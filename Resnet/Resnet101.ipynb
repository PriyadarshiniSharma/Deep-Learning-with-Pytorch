{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from ResNet import ResNet, ResNet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=128,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = ResNet101(10).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss [1, 1](epoch, minibatch):  0.07317168712615967\n",
      "Loss [1, 2](epoch, minibatch):  0.052230825424194334\n",
      "Loss [1, 3](epoch, minibatch):  0.04962517261505127\n",
      "Loss [1, 4](epoch, minibatch):  0.026245033740997313\n",
      "Loss [1, 5](epoch, minibatch):  0.03653975486755371\n",
      "Loss [1, 6](epoch, minibatch):  0.052273130416870116\n",
      "Loss [1, 7](epoch, minibatch):  0.06685672283172607\n",
      "Loss [1, 8](epoch, minibatch):  0.02310443639755249\n",
      "Loss [1, 9](epoch, minibatch):  0.07231516361236573\n",
      "Loss [1, 10](epoch, minibatch):  0.03223624467849731\n",
      "Loss [1, 11](epoch, minibatch):  0.027728211879730225\n",
      "Loss [1, 12](epoch, minibatch):  0.02800156831741333\n",
      "Loss [1, 13](epoch, minibatch):  0.03810319900512695\n",
      "Loss [1, 14](epoch, minibatch):  0.029915854930877686\n",
      "Loss [1, 15](epoch, minibatch):  0.02435231685638428\n",
      "Loss [1, 16](epoch, minibatch):  0.029880506992340086\n",
      "Loss [1, 17](epoch, minibatch):  0.032362701892852785\n",
      "Loss [1, 18](epoch, minibatch):  0.023562648296356202\n",
      "Loss [1, 19](epoch, minibatch):  0.030006036758422852\n",
      "Loss [1, 20](epoch, minibatch):  0.03300166130065918\n",
      "Loss [1, 21](epoch, minibatch):  0.048923921585083005\n",
      "Loss [1, 22](epoch, minibatch):  0.023752350807189942\n",
      "Loss [1, 23](epoch, minibatch):  0.022576885223388674\n",
      "Loss [1, 24](epoch, minibatch):  0.049430956840515135\n",
      "Loss [1, 25](epoch, minibatch):  0.030403752326965332\n",
      "Loss [1, 26](epoch, minibatch):  0.03423527002334595\n",
      "Loss [1, 27](epoch, minibatch):  0.02967148780822754\n",
      "Loss [1, 28](epoch, minibatch):  0.023063054084777834\n",
      "Loss [1, 29](epoch, minibatch):  0.03197207927703857\n",
      "Loss [1, 30](epoch, minibatch):  0.030867278575897217\n",
      "Loss [1, 31](epoch, minibatch):  0.02789118528366089\n",
      "Loss [1, 32](epoch, minibatch):  0.02702172040939331\n",
      "Loss [1, 33](epoch, minibatch):  0.023211100101470948\n",
      "Loss [1, 34](epoch, minibatch):  0.0413017988204956\n",
      "Loss [1, 35](epoch, minibatch):  0.026562259197235108\n",
      "Loss [1, 36](epoch, minibatch):  0.022751779556274415\n",
      "Loss [1, 37](epoch, minibatch):  0.03134285926818848\n",
      "Loss [1, 38](epoch, minibatch):  0.026315667629241944\n",
      "Loss [1, 39](epoch, minibatch):  0.05136053562164307\n",
      "Loss [1, 40](epoch, minibatch):  0.03586835145950317\n",
      "Loss [1, 41](epoch, minibatch):  0.033102402687072756\n",
      "Loss [1, 42](epoch, minibatch):  0.0434971284866333\n",
      "Loss [1, 43](epoch, minibatch):  0.04619234561920166\n",
      "Loss [1, 44](epoch, minibatch):  0.03046776294708252\n",
      "Loss [1, 45](epoch, minibatch):  0.04558851718902588\n",
      "Loss [1, 46](epoch, minibatch):  0.025672998428344727\n",
      "Loss [1, 47](epoch, minibatch):  0.02297511100769043\n",
      "Loss [1, 48](epoch, minibatch):  0.023620343208312987\n",
      "Loss [1, 49](epoch, minibatch):  0.056656408309936526\n",
      "Loss [1, 50](epoch, minibatch):  0.05507301330566406\n",
      "Loss [1, 51](epoch, minibatch):  0.023116452693939207\n",
      "Loss [1, 52](epoch, minibatch):  0.023063960075378417\n",
      "Loss [1, 53](epoch, minibatch):  0.022482497692108153\n",
      "Loss [1, 54](epoch, minibatch):  0.037398808002471924\n",
      "Loss [1, 55](epoch, minibatch):  0.03775054454803467\n",
      "Loss [1, 56](epoch, minibatch):  0.028105633258819582\n",
      "Loss [1, 57](epoch, minibatch):  0.031237223148345948\n",
      "Loss [1, 58](epoch, minibatch):  0.02383045673370361\n",
      "Loss [1, 59](epoch, minibatch):  0.025413877964019775\n",
      "Loss [1, 60](epoch, minibatch):  0.04557311534881592\n",
      "Loss [1, 61](epoch, minibatch):  0.024762277603149415\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c2be44195b29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    losses = []\n",
    "    running_loss = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for i, inp in enumerate(trainloader):\n",
    "        inputs, labels = inp\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i%1 == 0 and i > 0:\n",
    "            print(f'Loss [{epoch+1}, {i}](epoch, minibatch): ', running_loss / 100)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    avg_loss = sum(losses)/len(losses)\n",
    "    scheduler.step(avg_loss)\n",
    "            \n",
    "print('Training Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 10,000 test images:  13.120000000000001 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy on 10,000 test images: ', 100*(correct/total), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
